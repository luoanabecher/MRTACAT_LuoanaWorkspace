{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9106ef3-b532-41f2-a123-8ab55afc464d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Send Chat Messages to RTI\n",
    "Stream data from the groundingzavachats.txt file to RTI for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab9a0083-09c6-42cc-98d7-f99fab93d619",
   "metadata": {
    "cellStatus": "{\"Tyler Becker\":{\"session_start_time\":null,\"execution_start_time\":\"2025-06-22T05:08:44.1577866Z\",\"execution_finish_time\":\"2025-06-22T05:08:56.9675138Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Install the required package\n",
    "%pip install azure-cosmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "071e3896-3701-4511-ac94-459e00d2e179",
   "metadata": {
    "cellStatus": "{\"Tyler Becker\":{\"session_start_time\":null,\"execution_start_time\":\"2025-06-22T05:09:00.53131Z\",\"execution_finish_time\":\"2025-06-22T05:09:00.8236863Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import random\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "from azure.cosmos import CosmosClient, PartitionKey\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "import uuid\n",
    "from pyspark.sql.types import IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "380aab2e-3940-43f7-9f71-876f0ccf8182",
   "metadata": {
    "cellStatus": "{\"Tyler Becker\":{\"session_start_time\":null,\"execution_start_time\":\"2025-06-22T05:38:23.5960569Z\",\"execution_finish_time\":\"2025-06-22T05:38:25.8930167Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1 create the dataframe\n",
    "\n",
    "# Define the path to the CSV file in the Lakehouse\n",
    "file_path = \"abfss://Zava@onelake.dfs.fabric.microsoft.com/ZavaLakehouse.Lakehouse/Files/chatdata/groundingzavachats.txt\"\n",
    "\n",
    "# Read the text file into a DataFrame\n",
    "df = spark.read.text(file_path)\n",
    "\n",
    "# Rename the 'value' column to 'message'\n",
    "df = df.withColumnRenamed(\"value\", \"message\")\n",
    "\n",
    "# Define a UDF to generate a UUID\n",
    "uuid_udf = F.udf(lambda: str(uuid.uuid4()), StringType())\n",
    "\n",
    "# Add the ID column with UUIDs to the DataFrame\n",
    "df = df.withColumn(\"id\", uuid_udf())\n",
    "\n",
    "# Add an incrementing integer column to each row\n",
    "df = df.withColumn(\"chatid\", F.monotonically_increasing_id()+1)\n",
    "\n",
    "# Add a customer id that is random\n",
    "df = df.withColumn(\"customerid\", (F.rand() * 1000000).cast(\"int\"))\n",
    "\n",
    "# Add a initial sentiment score\n",
    "# Function to generate a random number with normal distribution, mean=5, std=1.75 and round it to the nearest integer\n",
    "def generate_random_normal_rounded():\n",
    "    return int(round(max(0, min(10, random.normalvariate(5, 1.75)))))\n",
    "# Register the UDF\n",
    "random_normal_udf = F.udf(generate_random_normal_rounded, IntegerType())\n",
    "# Add the new column with distributed sentiment\n",
    "df = df.withColumn(\"initialsentiment\", random_normal_udf())\n",
    "\n",
    "# Add datatime of chat\n",
    "df = df.withColumn(\"chatdatetime\", F.current_timestamp())\n",
    "\n",
    "# Show the first few rows\n",
    "df.show()\n",
    "\n",
    "# Step 2: Convert DataFrame to JSON rows\n",
    "json_rows = df.toJSON().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b8bd4f-d252-4c4c-8c68-ef62d61ce017",
   "metadata": {
    "cellStatus": "{\"Tyler Becker\":{\"session_start_time\":null,\"execution_start_time\":\"2025-06-22T05:40:05.3272229Z\",\"execution_finish_time\":\"2025-06-22T05:40:06.7507845Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Setup cosmos connection info\n",
    "COSMOS_DB_ENDPOINT = \"\"\n",
    "COSMOS_DB_KEY = \"\"\n",
    "DATABASE_NAME = \"zava\"\n",
    "CONTAINER_NAME = \"chatlog\"\n",
    "\n",
    "# Initialize the Cosmos client\n",
    "client = CosmosClient(COSMOS_DB_ENDPOINT, COSMOS_DB_KEY)\n",
    "\n",
    "# Create or get the database\n",
    "database = client.create_database_if_not_exists(id=DATABASE_NAME)\n",
    "\n",
    "# Create or get the container\n",
    "container = database.create_container_if_not_exists(\n",
    "    id=CONTAINER_NAME,\n",
    "    partition_key=PartitionKey(path=\"/id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f62a32-b004-4a6e-8d7d-b1cf011c2db2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Start the data generation below to send one round of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b40825ca-6e82-4c0d-ab18-3678b82e758c",
   "metadata": {
    "cellStatus": "{\"Tyler Becker\":{\"session_start_time\":null,\"execution_start_time\":\"2025-06-22T05:38:32.0427008Z\",\"execution_finish_time\":\"2025-06-22T05:38:58.3520102Z\",\"state\":\"finished\",\"livy_statement_state\":\"cancelled\",\"normalized_state\":\"cancelled\"}}",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Write each row to the database once\n",
    "counter=1\n",
    "while True:\n",
    "    for row in json_rows:\n",
    "        container.create_item(body=json.loads(row))\n",
    "        print(f\"Inserted row: {counter}\")\n",
    "        time.sleep(1)\n",
    "        counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e41d818f-9999-4f9a-8a9b-45b5d8c74b96",
   "metadata": {
    "cellStatus": "{\"Tyler Becker\":{\"session_start_time\":null,\"execution_start_time\":\"2025-06-21T22:47:17.5076586Z\",\"execution_finish_time\":\"2025-06-21T22:48:48.3292438Z\",\"state\":\"finished\",\"livy_statement_state\":\"cancelled\",\"normalized_state\":\"cancelled\"}}",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4b: Keep writing the file to the database (need to recreate ids for CosmosDB)\n",
    "counter=1\n",
    "while True:\n",
    "    # Read the text file into a DataFrame\n",
    "    df = spark.read.text(file_path)\n",
    "    print(\"File Read\")\n",
    "\n",
    "    # Rename the 'value' column to 'message'\n",
    "    df = df.withColumnRenamed(\"value\", \"message\")\n",
    "\n",
    "    # Define a UDF to generate a UUID\n",
    "    uuid_udf = F.udf(lambda: str(uuid.uuid4()), StringType())\n",
    "\n",
    "    # Add the ID column with UUIDs to the DataFrame\n",
    "    df = df.withColumn(\"id\", uuid_udf())\n",
    "\n",
    "    # Add an incrementing integer column to each row\n",
    "    df = df.withColumn(\"chatid\", F.monotonically_increasing_id()+1)\n",
    "\n",
    "    # Add a customer id that is random\n",
    "    df = df.withColumn(\"customerid\", (F.rand() * 1000000).cast(\"int\"))\n",
    "\n",
    "    # Add a initial sentiment score\n",
    "    df = df.withColumn(\"initialsentiment\", (F.rand() * 11).cast(\"int\"))\n",
    "\n",
    "    # Add datatime of chat\n",
    "    df = df.withColumn(\"chatdatetime\", F.current_timestamp())\n",
    "\n",
    "    json_rows = df.toJSON().collect()\n",
    "\n",
    "    for row in json_rows:\n",
    "        container.create_item(body=json.loads(row))\n",
    "        print(f\"Inserted row: {counter}\")\n",
    "        time.sleep(1)\n",
    "        counter += 1\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
